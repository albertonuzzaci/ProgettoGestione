{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "## *Table of contents*\n",
    "* *Setup Phase*\n",
    "\t1. Import libraries and queries\n",
    "\t2. Showing all the available queries\n",
    "\t3. Query selection\n",
    "\n",
    "* *Evaluation Phase*\n",
    "\t1. Precision at Standard Recall Levels for query Q\n",
    "\t2. Interpolated Average Precision (IAP) at Standard Recall Leveles\n",
    "\t3. R-Precision\n",
    "\t4. Mean Average Precision (MAP)\n",
    "\t5. F-Measure & E-Measure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Setup phase*\n",
    "\n",
    "### Import libraries and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing all the available queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    examined_q = 3\n",
    "    print(\"User Information Need: \" + queries[examined_q][\"UIN\"])\n",
    "except IndexError as e:\n",
    "    print(\"index not valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.functions import Benchmark\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Suppress the warning \n",
    "\n",
    "b = Benchmark(queries[examined_q])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision at Standard Recall Levels for query Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define axes' names\n",
    "axes = [\"recall\", \"precision\"]\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame()\n",
    "for model, model_name in models:\n",
    "    result = b.getResults(20, model)\n",
    "    # get precision at standard recall values over list of result\n",
    "    SRLValues = b.getSRLValues(\n",
    "        b.getPrecisionValues(result),\n",
    "        b.getRecallValues(result)\n",
    "    )\n",
    "    \n",
    "    # tmp dataframe concatenated to the main one\n",
    "    dfB = pd.DataFrame(SRLValues, columns = axes)\n",
    "    dfB[\"Version\"] = f'{model_name}'\n",
    "    \n",
    "    df = pd.concat([df, dfB])\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "# plot the line graph\n",
    "pltP = sns.lineplot(data = df, x = 'recall', y = 'precision', marker='o', markersize=4, hue=\"Version\", palette=\"colorblind\")\n",
    "pltP.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltP.set_xlim([-0.1, 1.1]);\n",
    "pltP.set_ylim([-0.1, 1.1]);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAPatSRL = {}\n",
    "\n",
    "for q in queries:\n",
    "    tmpB = Benchmark(q)\n",
    "    for model, model_name in models:\n",
    "        result = tmpB.getResults(20, model)\n",
    "        SRLValues = tmpB.getSRLValues(\n",
    "            tmpB.getPrecisionValues(result),\n",
    "            tmpB.getRecallValues(result)\n",
    "            )\n",
    "        IAPatSRL.setdefault(model_name, []).append(\n",
    "\t\t\tSRLValues\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolated Average Precision (IAP) at Standard Recall Leveles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "import textwrap\n",
    "\n",
    "versions = [] \n",
    "AvPr_values=[]\n",
    "\n",
    "for model, model_name in models:\n",
    "    result = b.getResults(20, model)\n",
    "    SRLValues = b.getSRLValues(\n",
    "        b.getPrecisionValues(result),\n",
    "        b.getRecallValues(result)\n",
    "    )\n",
    "    \n",
    "    AvPr_values.append(b.getIapAvgPrecision(SRLValues))\n",
    "    \n",
    "    versions.append(textwrap.fill(model_name, width=10,\n",
    "                    break_long_words=True))\n",
    "    \n",
    "# plot the average precisions\n",
    "# apply the default theme\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame({\"Search-engine version\": versions, \"IAP at SRL\": AvPr_values})\n",
    "\n",
    "# plot the bar graph\n",
    "pltAvPr = sns.barplot(data = df, x = \"Search-engine version\", y = 'IAP at SRL',palette=\"colorblind\")\n",
    "\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltAvPr.set_ylim([0.0, max(AvPr_values)+0.20]); # set y-axis    \n",
    "pltAvPr.yaxis.set_major_locator(MultipleLocator(0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many relevant documents I have returned compared to the ideal case in which in the first $n$ documents are all relevants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Precision Comparison between two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Available models: \n",
    "- \"BM25F\"\n",
    "- \"Doc2Vec\"\n",
    "- \"Sentiment Weighting Model\"\n",
    "- \"Sentiment Weighting Model - Amount Reviews Based\"\n",
    "'''\n",
    "\n",
    "model1 = \"BM25F\"\n",
    "model2 = \"Doc2Vec\"\n",
    "\n",
    "for model, model_name in models:\n",
    "    if model1 == model_name:\n",
    "        model1 = model\n",
    "    if model2 == model_name:\n",
    "        model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_comparison = []\n",
    "for q in queries:\n",
    "    tmpB = Benchmark(q)\n",
    "    model1Res = tmpB.getResults(20, model1)\n",
    "    model2Res = tmpB.getResults(20, model2)\n",
    "    \n",
    "    RP_comparison.append(\n",
    "        tmpB.getRPrecision(model1Res) - tmpB.getRPrecision(model2Res)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, get Non-Interpolated Average Precision for each query using $ \\sum_{r=0}^{n} \\frac{P_q(r/|R_q|)}{|R_q|} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, compute Mean Average Precision for every model which is just an average between all the NIAP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure & E-Measure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E-measure is a variant of the harmonic mean which allows us to emphasize the value of recall or precision based on what we are interested in:\n",
    "\n",
    "- $b=1 \\rightarrow 1-\\text{F-Measure}$ \n",
    "- $b>1$ emphasize precision\n",
    "- $b<1$ emphasize recall\n",
    "\n",
    "*Precision or Recall?*\n",
    "\n",
    "- High Recall: relevant documents, but with too many unrelevant documents. \n",
    "- High Precision: few results but with an greater probability of being relevant. \n",
    "\n",
    "It's possible to customize *b* value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
